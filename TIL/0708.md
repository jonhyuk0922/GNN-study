Title : Graph Augmentation for GNNs

- GNN layer를 통해 학습을 하기 위해 graph augmentation이 필요한 경우들이 있다. 즉, raw input graph 이 게산가능한 그래프가 아니라는 전제하에 출발한다.
- 우선 augmentation 에는 두가지가 있는데 1. Graph feature augmentation 2. Graph structure augmentation 이 있다.
- 그렇다면 우리가 augmentation을 왜 하는 가?! 당연히 그래프가 계산가능하여 GNN을 통해 학습할 수 있는 상태로 만들기 위함이다. 
- 피처가 부족한 경우 (노드 피처가 없는 경우, 순환하는 구조) / 그래프 구조가 너무 sparse하거나 dense하거나 큰 경우에 augmentation이 필요하다. 

1. 피처가 부족한 경우 
우선 노드 피처가 없는 경우엔 두 가지 방안이 있는데 각 노드별로 상수를 할당해주는 것과 유니크한 ID값을 할당해주는 것이다.
위 두가지 방법은 장단점이 있는데, 각 노드별 상수를 할당해주는 방법은 계산비용도 적게들고 일반화도 잘되서 여러 케이스에 사용할 수 잇으나 표현함에 있어서 두번째 방법보다 못하다.
두번째 방법으로 유니크한 값(원 핫 인코딩)하여 노드 피처를 주변 표현력은 좋지만 주어진 그래프를 꼬아서 다른 모양으로 한 경우 외에 다른 그래프에는 적용이 어렵다는 단점이 있다

순환하는 구조는 사실상 길이가 3인 순환구조의 v1노드와 길이가 4인 순환구조의 v2노드가 분명히 다름에도 구분할 수가없다. 왜냐면 모든 노드가 degree 2를 가지고 있으며 이웃노드들 또한 동일하다.
computational 그래프도 모든게 똑같다. 따라서 GNN이 학습할 수 없다. 이를 해결하기 위해 사이클 길이를 카운트하는 노드 피처를 augment해준다.
예를 들어, 길이가 3인 순환구조면 [0,0,0,1,0,0] 인 노드피처를 추가해주고 , 길이가 4면 [0,0,0,0,1,0] 을 추가해준다. 그 외에도 node degree , clustering coef , pagerank , centrality 등을 추가해줄 수 있다.

2. 그래프 구조가 너무 sparse or dense or large 한 경우
- 첫번째로 sparse한 경우 가상 엣지를 추가해준다. 직관은 인접행렬로 A 대신 A+A제곱을 사용하는 것인데 사실 이것보단 예시가 와닿는다. 
예를 들어 이분 그래프(Bipartite graphs)의 경우 사용되는데, 저자와 논문간의 관계를 표현할 때 2 hop 가상 엣지를 사용하면 공동 저자를 더 효율적으로 표현할 수 있다. 
- 두번째 방법으로 가상 노드를 추가해줄 수 있다. 그림을 보는 게 이해가 잘되긴하는데, sparse한 그래프에서 끝과끝으로 메시지를 보낸다고 생각하자. 가상의 저~위 노드를 하나만들어서 그걸 통하면 훨씬 효율적으로 메세지를 전달 할 수 있다.
- 마지막으로 너무 큰 그래프일때 이웃 샘플링을 할 수 있다. 예를 들어, 내가 셀럽이라 팔로워가 100만명인데 그 100만명의 메세지를 다 받을 수 없으니, 내 가족들과의 그래프만 선택해서 메세지를 주고받는 것이다. 
그래프를 보면 타겟노드에 3개의 이웃노드가 있을 때 랜덤하게 2개만 골라서 메세지가 타겟노드로 통하게 한다. 다만 내가 선택하지 않은 이웃노드를 통해 중요한 메세지가 들어올 수도 있지만, 이렇게 샘플링하면 크기를 줄일 수 있어서 계산하기 좋다. 결과적으로 잘 작동한다고 한다. 나중에 추천시스템에서 적용해보고 싶다. 
